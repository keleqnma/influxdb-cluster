## 1.项目配置中心

项目采用etcd集群作为配置中心，负责存储每个单独Influx数据库的元数据（如URL，Zone，Timeout，checkInterval等信息），以及集群协调节点的元数据，etcd集群使用raft一致性协议保证集群的稳定性，也保证了重要元数据的稳定性，在部分etcd存储节点网络波动或宕机时，能很好的保证节点数据存储的稳定性，以及数据库集群重启后能快速获取元数据完成数据重启。该部分的单节点配置存储和写入的开发与测试均已完成。

## 2. 集群协调功能

集群协调节点基于高性能web开发框架Ginonic对外提供http接口，省去了自己包装http接口的一些芜杂细节，也保证了对外接口的稳定性。集群协调节点提供的功能兼容美团开源项目influx-proxy，如写，读，重启节点，也新增了一个功能，即新增新Influx数据库节点的接口，该项目采用etcd集群作为配置中心，所以支持对实际Influx数据库节点的动态伸缩。集群协调提供的写，读接口兼容Influxql查询写入格式。集群协调节点对后续backend db的负载均衡转发策略目前采用较为简单的轮询式，这部分尚在开发当中。

### 2.1. Influxql语句解析

Influxql格式解析，为每一个查询语句生成一个buffer区，以此节省操作string语句的开销，该功能用于解析提取查询语句的Measurement（类似Mysql查询语句里的table），该功能部分的开发以及测试均已完成，

### 2.2. Backend  db接口部分

该部分为上层集群协调提供一个统一接口（写，读），每一个实际存在的influx数据库节点对应一个backend db，当注册一个backend db时，后台会启动一个goroutine，对注册的Influx数据库节点提供健康检查，间隔规定的时间对backend influxdb 发起Ping请求。此外，写入数据默认采用gzip格式压缩，节省存储空间。

backend部分对实际的Influx数据库节点的写入并非每次写入都实际写入，会先写入到本地buffer区，间隔一定时间将buffer区写入实际的Influx数据库节点，或者当数据超过一定大小后，也会刷新buffer区，将数据写入，并且重置定时器，该功能部分的开发以及测试均已完成。

## 日志系统

日志系统采用高性能日志框架Zaplogger，能节省服务器花费在打印服务日志上的内存以及cpu。项目采用自定义日志格式，严格区分info，error，fatal级别，打印日志代码行数以及函数日志栈，为后续兼容更大更专的日志检索系统，如ElasticSerach，提供了良好的兼容性及伸缩性。

## 项目测试

### 功能测试

代码开发方式采用TDD式，即测试驱动开发，先写测试再实现功能，保证了开发的每一步路上开发的功能都满足预期，下图是项目的部分功能测试结果。

![image-20210226213613532](/Users/chenqiqi/Library/Application Support/typora-user-images/image-20210226213613532.png)

### Bench测试

目前Bench测试完成了Influxql解析部分的，其他部分的bench测试还亟需开发。

![image-20210227114721251](/Users/chenqiqi/Library/Application Support/typora-user-images/image-20210227114721251.png)

### 项目可视化

项目可视化部分使用Grafana，Grafana是一款用Go语言开发的开源数据可视化工具，可以做数据监控和数据统计，带有告警功能，原生支持InfluxDB数据源。



在开发 influxdb-cluster时，阅读美团开源influx-proxy（现在人气较高的集群Influx开源版）源码时，发现其在对backend db写入失败时，采用往文件里写入数据，再读取文件定期重试的方法，考虑到业务场景（轴承错误数据）后，写入失败是小概率事件，且将失败数据写入文件需要承担文件系统损坏的风险，本项目需要了另外形式的策略：放弃在代理层失败重试，写入失败时直接返回请求数据和错误信息，让应用层处理数据而不是在代理层处理。